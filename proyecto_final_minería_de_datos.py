# -*- coding: utf-8 -*-
"""Proyecto Final - Minería De Datos

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18ZIGA4a5wmmPtpXXMoZ6EdIuoY63qFx5

"""

"""#1 - Análisis Exploratorio de Datos (EDA)

###1.1. Cargar el dataset y revisar columnas, tipos de datos y valores nulos
"""

# Montar Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Cargar el archivo desde la ruta en Drive
import pandas as pd

ruta_archivo = "/content/drive/My Drive/MineriaDatos/ProyectoFinal/Nuevo_Dataset_Patrones_Emocionales.csv"
df = pd.read_csv(ruta_archivo)

# Mostrar las primeras filas para inspección visual inicial
print("Vista previa de los primeros registros:")
display(df.head())

# Ver tipos de datos por columna
print("\nTipos de datos por columna:")
print(df.dtypes)

# Verificar valores nulos por columna
print("\nConteo de valores nulos por columna:")
print(df.isnull().sum())

# Mostrar la forma del DataFrame (filas, columnas)
print(f"\nEl dataset contiene {df.shape[0]} filas y {df.shape[1]} columnas.")

"""###1.2 Análisis de cantidad de datos por pregunta y emoción"""

import pandas as pd

# Diccionario de mapeo de preguntas a emociones
emociones_por_pregunta = {
    '1': 'Felicidad',
    '2': 'Felicidad',
    '3': 'Tristeza',
    '4': 'Tristeza',
    '5': 'Disgusto',
    '6': 'Disgusto',
    '7': 'Ira',
    '8': 'Ira',
    '9': 'Miedo',
    '10': 'Miedo',
    '11': 'Sorpresa',
    '12': 'Sorpresa'
}

# Extraer columnas de las preguntas (del 1 al 12)
columnas_preguntas = [col for col in df.columns if col.strip().startswith(tuple(emociones_por_pregunta.keys()))]

# Conteo de respuestas válidas por pregunta
conteo_por_pregunta = df[columnas_preguntas].notnull().sum().rename(lambda c: c.strip())

print("Cantidad de respuestas válidas por pregunta:")
display(conteo_por_pregunta)

# Construir conteo por emoción sumando las preguntas correspondientes
from collections import defaultdict

conteo_emociones = defaultdict(int)

for col in columnas_preguntas:
    col_limpia = col.strip()
    num_pregunta = col_limpia.split('.')[0]
    emocion = emociones_por_pregunta.get(num_pregunta)
    if emocion:
        conteo_emociones[emocion] += df[col].notnull().sum()

# Convertir a DataFrame para presentación
df_conteo_emociones = pd.DataFrame.from_dict(conteo_emociones, orient='index', columns=['Cantidad de respuestas'])
df_conteo_emociones = df_conteo_emociones.sort_values(by='Cantidad de respuestas', ascending=False)

print("\nCantidad total de respuestas por categoría de emoción:")
display(df_conteo_emociones)

df.isnull().sum()

"""###1.3 Visualización de la distribución de emociones"""

import matplotlib.pyplot as plt
import seaborn as sns

# Asegurar estilos
plt.style.use('ggplot')
sns.set(font_scale=1.1)

# Preparar el DataFrame para graficar
df_plot = df_conteo_emociones.reset_index().rename(columns={'index': 'Emoción'})

plt.figure(figsize=(10, 6))
sns.barplot(
    data=df_plot,
    x='Emoción',
    y='Cantidad de respuestas',
    hue='Emoción',
    palette='Set2',
    dodge=False,
    legend=False
)

plt.title("Distribución de respuestas por categoría de emoción", fontsize=16)
plt.xlabel("Emoción", fontsize=12)
plt.ylabel("Cantidad de respuestas", fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""###1.4. Generar nubes de palabras por categoría de emoción"""

from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd
from collections import defaultdict

# Lista personalizada de stopwords en español
stopwords_es = set(STOPWORDS).union({
    "que", "de", "y", "a", "el", "en", "la", "lo", "me", "un", "por", "con", "no", "se", "al", "del", "ya", "mi", "te",
    "eso", "pero", "sí", "si", "era", "fue","cuando", "como", "para", "una", "las", "los",
    "porque", "pues", "hay", "yo", "tu", "sus", "tus", "ese", "esa", "eso", "entonces", "también", "aunque", "les", "es", "o", "le"
})

# --- Código para generar respuestas_por_emocion ---
# Diccionario de mapeo de preguntas a emociones (repetimos para claridad en este bloque)
emociones_por_pregunta = {
    '1': 'Felicidad', '2': 'Felicidad',
    '3': 'Tristeza',  '4': 'Tristeza',
    '5': 'Disgusto',  '6': 'Disgusto',
    '7': 'Ira',       '8': 'Ira',
    '9': 'Miedo',     '10': 'Miedo',
    '11': 'Sorpresa','12': 'Sorpresa'
}

# Recolectar respuestas por emoción
respuestas_por_emocion = defaultdict(list)

try:
    columnas_preguntas
except NameError:
    columnas_preguntas = [col for col in df.columns if col.strip().startswith(tuple(emociones_por_pregunta.keys()))]


for col in columnas_preguntas:
    col_limpia = col.strip()
    num_pregunta = col_limpia.split('.')[0]
    emocion = emociones_por_pregunta.get(num_pregunta)
    if emocion:
        # Filtrar valores nulos y convertirlos a lista para la emoción actual
        textos_validos = df[col].dropna().astype(str).tolist()
        respuestas_por_emocion[emocion].extend(textos_validos)

# Mostrar cuántos textos hay por emoción

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()

for i, (emocion, textos) in enumerate(respuestas_por_emocion.items()):
    # Unir todos los textos de la emoción en una sola cadena
    texto_completo = " ".join(textos)

    # Generar la nube de palabras solo si hay texto disponible
    if texto_completo.strip(): # Evita generar nubes vacías
        nube = WordCloud(
            width=800,
            height=400,
            background_color='white',
            stopwords=stopwords_es,   # ← Aquí se filtran solo para la nube
            collocations=False
        ).generate(texto_completo)

        axes[i].imshow(nube, interpolation='bilinear')
    else:
        # Si no hay texto, simplemente apagar el eje y poner un título
        axes[i].text(0.5, 0.5, "No hay datos para esta emoción",
                     horizontalalignment='center', verticalalignment='center',
                     fontsize=12, color='gray')

    axes[i].axis('off')
    axes[i].set_title(emocion, fontsize=16)

# Ocultar ejes sobrantes si hay menos de 6 emociones
for j in range(len(respuestas_por_emocion), len(axes)):
    fig.delaxes(axes[j])


plt.tight_layout()
plt.show()

"""###1.5 Generar diagramas de frecuencias (top palabras más comunes por emoción)"""

from collections import Counter
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import re

def limpiar_texto(texto):
    texto = texto.lower()
    texto = re.sub(r'[^a-záéíóúüñ\s]', '', texto)
    palabras = texto.split()
    return [p for p in palabras if p not in stopwords_es]

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()

for i, (emocion, textos) in enumerate(respuestas_por_emocion.items()):
    palabras = []
    for texto in textos:
        palabras.extend(limpiar_texto(texto))

    top_palabras = Counter(palabras).most_common(10)
    palabras_df = pd.DataFrame(top_palabras, columns=['Palabra', 'Frecuencia'])

    sns.barplot(
        data=palabras_df,
        x='Frecuencia',
        y='Palabra',
        hue='Palabra',
        dodge=False,
        palette='viridis',
        legend=False,
        ax=axes[i]
    )

    axes[i].set_title(f"Top 10 palabras frecuentes - {emocion}", fontsize=14)
    axes[i].set_xlabel("Frecuencia")
    axes[i].set_ylabel("Palabra")

plt.tight_layout()
plt.show()

"""#2. Preprocesamiento de Texto

##2.1. Filtrado de respuestas no útiles o vacías
"""

# Detectar columnas que son preguntas emocionales (1. a 12.)
prefijos = tuple(f"{i}." for i in range(1, 13))
columnas_emocionales = [col for col in df.columns if col.strip().startswith(prefijos)]

# Lista de respuestas que no aportan información útil
respuestas_vacias = {
    "", ".", "..", "...", "no", "nada", "ninguna", "ninguno", "na", "no recuerdo",
    "no sé", "nose", "x", "-", "ningún"
}

# Función para saber si una respuesta es válida
def es_valiosa(texto):
    if not isinstance(texto, str):
        return False
    texto = texto.strip().lower()
    if texto in respuestas_vacias:
        return False
    if len(texto.split()) < 3:
        return False
    return True

# Función que cuenta respuestas útiles por fila
def respuestas_validas_en_fila(fila):
    return sum([es_valiosa(fila[col]) for col in columnas_emocionales])

# Umbral mínimo de respuestas útiles por fila
minimo_validas = 10

# Aplicar el filtro
df_filtrado = df[df.apply(respuestas_validas_en_fila, axis=1) >= minimo_validas].copy()

# Ver cuántas filas quedan
print(f"Filas antes del filtrado: {df.shape[0]}")
print(f"Filas después del filtrado: {df_filtrado.shape[0]}")

"""###2.2. Corrección gramatical automática"""

import language_tool_python
import pandas as pd
import re

# Inicializar el corrector
corrector = language_tool_python.LanguageTool('es')

# Crear copia del DataFrame original
df_corregido = df_filtrado.copy()

# Función segura que protege la palabra BUAP antes de corregir
def corregir_texto(texto):
    if not isinstance(texto, str) or texto.strip() == "":
        return texto
    try:
        # Proteger variantes de BUAP (buap, Buap, BUAP)
        texto_protegido = re.sub(r'\bbuap\b', '__BUAP__', texto, flags=re.IGNORECASE)

        # Corregir con language_tool
        texto_corregido = corrector.correct(texto_protegido)

        # Restaurar BUAP
        texto_final = texto_corregido.replace('__BUAP__', 'BUAP')

        return texto_final
    except Exception as e:
        print("Error corrigiendo:", texto[:30], "-", str(e))
        return texto  # Devolver el original si algo falla

# Aplicar la corrección gramatical a las columnas emocionales
for columna in columnas_emocionales:
    nueva_columna = columna.replace(".", "") + "_corr"
    print(f"Corrigiendo columna: {columna} → {nueva_columna}")
    df_corregido[nueva_columna] = df_corregido[columna].apply(corregir_texto)

# Verificación visual con un ejemplo
col_ejemplo = columnas_emocionales[0]
col_corregida = col_ejemplo.replace(".", "") + "_corr"
print("\nEjemplo de corrección gramatical (sin modificar 'BUAP'):")
display(df_corregido[[col_ejemplo, col_corregida]].sample(5))

"""###2.3. Limpieza de texto"""

import re

# ─────────── LIMPIEZA PARA TF-IDF  ───────────
df_limpio = df_corregido.copy()

def limpiar_texto_basico(texto):
    if not isinstance(texto, str):
        return ""
    texto = texto.lower()
    texto = re.sub(r'[^a-záéíóúñü\s]', '', texto)
    texto = re.sub(r'\s+', ' ', texto).strip()
    return texto

for col in columnas_emocionales:
    col_corregida = col.replace(".", "") + "_corr"
    col_limpia = col.replace(".", "") + "_limpio"
    df_limpio[col_limpia] = df_limpio[col_corregida].apply(limpiar_texto_basico)
    print(f"Limpieza aplicada: {col_corregida} → {col_limpia}")

# Verificación visual
col_ejemplo = columnas_emocionales[0].replace(".", "")
print("Ejemplo de limpieza para TF-IDF:")
display(df_limpio[[f"{col_ejemplo}_corr", f"{col_ejemplo}_limpio"]].sample(5))


# ─────────── LIMPIEZA PARA EMBEDDINGS (ligera y natural) ───────────
df_para_embeddings = df_corregido.copy()

def limpiar_texto_para_embeddings(texto):
    if not isinstance(texto, str):
        return ""
    texto = texto.strip()
    texto = re.sub(r'\s+', ' ', texto)  # Solo quitar espacios duplicados
    return texto  # Se mantiene puntuación, tildes, etc.

for col in columnas_emocionales:
    col_corregida = col.replace(".", "") + "_corr"
    col_emb = col.replace(".", "") + "_emb"
    df_para_embeddings[col_emb] = df_para_embeddings[col_corregida].apply(limpiar_texto_para_embeddings)
    print(f"Limpieza EMB aplicada: {col_corregida} → {col_emb}")

# Verificación visual
print("Ejemplo de limpieza suave para EMBEDDINGS:")
display(df_para_embeddings[[f"{col_ejemplo}_corr", f"{col_ejemplo}_emb"]].sample(5))

"""###2.4. Tokenización de texto"""

# Cargar spaCy en español
import spacy
nlp = spacy.load("es_core_news_sm")

# Crear nueva copia para seguir trabajando
df_tokenizado = df_limpio.copy()

# Función para tokenizar
def tokenizar_texto(texto):
    if not isinstance(texto, str) or texto.strip() == "":
        return []
    doc = nlp(texto)
    return [token.text for token in doc if not token.is_space]

# Aplicar tokenización a las columnas limpias
for col in columnas_emocionales:
    col_limpio = col.replace(".", "") + "_limpio"
    col_tokens = col.replace(".", "") + "_tokens"
    df_tokenizado[col_tokens] = df_tokenizado[col_limpio].apply(tokenizar_texto)
    print(f"Tokenización aplicada: {col_limpio} → {col_tokens}")

# Verificación de resultados
col_ejemplo = columnas_emocionales[0].replace(".", "")
print("Ejemplo de tokenización:")
display(df_tokenizado[[f"{col_ejemplo}_limpio", f"{col_ejemplo}_tokens"]].sample(5))

"""###2.5 Eliminación de stopwords"""

import nltk
from nltk.corpus import stopwords

# Descargar stopwords si no están
nltk.download('stopwords')

# Cargar lista base de stopwords en español
stopwords_es = set(stopwords.words('spanish'))

# Palabras que debemos conservar por su valor emocional
palabras_emocionales = {
    "no", "nunca", "jamás", "yo", "me", "mi", "mí", "tú", "te", "sí", "sentí", "sentía", "fue", "cuando", "porque", "nada", "todo", "bien", "mal"
}

# Crear lista filtrada
stopwords_personalizadas = stopwords_es - palabras_emocionales

# Función para eliminar stopwords de una lista de tokens
def quitar_stopwords(tokens):
    return [token for token in tokens if token.lower() not in stopwords_personalizadas]

# Crear nuevo DataFrame basado en los tokens
df_sin_stopwords = df_tokenizado.copy()

# Aplicar eliminación de stopwords a cada columna tokenizada
for col in columnas_emocionales:
    col_tokens = col.replace(".", "") + "_tokens"
    col_clean = col.replace(".", "") + "_tokens_clean"
    df_sin_stopwords[col_clean] = df_sin_stopwords[col_tokens].apply(quitar_stopwords)
    print(f"Stopwords eliminadas: {col_tokens} → {col_clean}")

# Verificación de ejemplo
col_ejemplo = columnas_emocionales[0].replace(".", "")
display(df_sin_stopwords[[f"{col_ejemplo}_tokens", f"{col_ejemplo}_tokens_clean"]].sample(5))

"""###2.6 Lematización con spaCy"""

import spacy

# Cargar modelo de idioma español
import es_core_news_sm
nlp = es_core_news_sm.load()

# Función para lematizar una lista de tokens
def lematizar_tokens(tokens):
    doc = nlp(" ".join(tokens))
    return [token.lemma_ for token in doc]

# Crear nuevo DataFrame para los lemas
df_lematizado = df_sin_stopwords.copy()

# Aplicar lematización a cada columna con tokens limpios
for col in columnas_emocionales:
    col_clean = col.replace(".", "") + "_tokens_clean"
    col_lemmas = col.replace(".", "") + "_lemmas"
    df_lematizado[col_lemmas] = df_lematizado[col_clean].apply(lematizar_tokens)
    print(f"Lematización aplicada: {col_clean} → {col_lemmas}")

# Ver ejemplo de lematización
col_ejemplo = columnas_emocionales[0].replace(".", "")
display(df_lematizado[[f"{col_ejemplo}_tokens_clean", f"{col_ejemplo}_lemmas"]].sample(5))

"""### 2.7. Etiquetado de Sentimientos"""

# Función para convertir lista de lemas a texto plano
def lemas_a_texto(lemas):
    return " ".join(lemas) if isinstance(lemas, list) else ""

# Mapeo de preguntas a emociones según las instrucciones
emociones_por_pregunta = {
    '1': 'Felicidad', '2': 'Felicidad',
    '3': 'Tristeza',  '4': 'Tristeza',
    '5': 'Disgusto',  '6': 'Disgusto',
    '7': 'Ira',       '8': 'Ira',
    '9': 'Miedo',     '10': 'Miedo',
    '11': 'Sorpresa', '12': 'Sorpresa'
}

# Lista para almacenar filas con texto y su etiqueta emocional
filas_etiquetadas = []

# Generar las filas desde df_lematizado
for _, fila in df_lematizado.iterrows():
    emocion_contenido = {}
    for col in columnas_emocionales:
        num_pregunta = col.split(".")[0]
        emocion = emociones_por_pregunta.get(num_pregunta)
        lemas = fila[col.replace(".", "") + "_lemmas"]
        if isinstance(lemas, list):
            emocion_contenido.setdefault(emocion, []).extend(lemas)

    # Agregar una fila por emoción con todos los lemas concatenados
    for emocion, lemas in emocion_contenido.items():
        if len(lemas) > 0:
            filas_etiquetadas.append({
                "texto": lemas_a_texto(lemas),
                "etiqueta_emocional": emocion
            })

# Crear DataFrame final etiquetado
df_etiquetado = pd.DataFrame(filas_etiquetadas)

# Mostrar muestra del DataFrame final
print("Ejemplo de filas con texto lematizado y etiqueta emocional:")
display(df_etiquetado.sample(5))

"""### 2.8. Vectorización de texto (TF-IDF)"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inicializar el vectorizador TF-IDF
vectorizador = TfidfVectorizer()

# Ajustar y transformar los textos en vectores numéricos
X_tfidf = vectorizador.fit_transform(df_etiquetado["texto"])

# Mostrar dimensiones de la matriz
print("Matriz TF-IDF generada con forma:", X_tfidf.shape)

# Opcional: mostrar términos más relevantes del primer documento
import pprint

i = 0  # índice de ejemplo
vector = X_tfidf[i]
feature_names = vectorizador.get_feature_names_out()
vector_dict = {feature_names[idx]: vector[0, idx] for idx in vector.nonzero()[1]}

print(f"\nTop 15 términos por peso en el documento {i}:")
pprint.pprint(sorted(vector_dict.items(), key=lambda x: -x[1])[:15])

"""#3. Generación de Embeddings usando Transformers"""

from sentence_transformers import SentenceTransformer
import numpy as np
import pandas as pd
from tqdm import tqdm

# Cargar modelo MiniLM multilingüe optimizado para embeddings
modelo_minilm = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
modelo_embeddings = SentenceTransformer(modelo_minilm)

# Construir nuevo DataFrame para embeddings (desde df_para_embeddings)
filas_embeddings = []

for _, fila in df_para_embeddings.iterrows():
    emocion_contenido = {}
    for col in columnas_emocionales:
        num_pregunta = col.split(".")[0]
        emocion = emociones_por_pregunta.get(num_pregunta)
        texto_emb = fila[col.replace(".", "") + "_emb"]
        if isinstance(texto_emb, str) and texto_emb.strip():
            emocion_contenido.setdefault(emocion, []).append(texto_emb.strip())

    for emocion, textos in emocion_contenido.items():
        if len(textos) > 0:
            filas_embeddings.append({
                "texto": " ".join(textos),
                "etiqueta_emocional": emocion
            })

# Crear DataFrame final para embeddings
df_embeddings_final = pd.DataFrame(filas_embeddings)

# Obtener textos y etiquetas
textos = df_embeddings_final["texto"].tolist()
etiquetas = df_embeddings_final["etiqueta_emocional"].tolist()

# Generar embeddings con MiniLM
X_use = modelo_embeddings.encode(textos, batch_size=32, show_progress_bar=True, convert_to_numpy=True)

print("Embeddings generados con MiniLM - forma:", X_use.shape)

"""# 4. División del Dataset"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pandas as pd

# ────── Codificar etiquetas para TF-IDF ──────
le_tfidf = LabelEncoder()
y_tfidf = le_tfidf.fit_transform(df_etiquetado["etiqueta_emocional"])

print("Distribución original de clases (TF-IDF):")
display(df_etiquetado["etiqueta_emocional"].value_counts())

# División TF-IDF
print("\nDividiendo conjunto para TF-IDF (80% train, 20% test)...")
X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(
    X_tfidf,
    y_tfidf,
    test_size=0.2,
    stratify=y_tfidf,
    random_state=42
)

# ────── Codificar etiquetas para EMBEDDINGS ──────
le_use = LabelEncoder()
y_use = le_use.fit_transform(df_embeddings_final["etiqueta_emocional"])

print("\nDistribución original de clases (Embeddings):")
display(df_embeddings_final["etiqueta_emocional"].value_counts())

# División MiniLM
print("\nDividiendo conjunto para MiniLM Embeddings (80% train, 20% test)...")
X_use_train, X_use_test, y_use_train, y_use_test = train_test_split(
    X_use,
    y_use,
    test_size=0.2,
    stratify=y_use,
    random_state=42
)

# ────── Función para mostrar distribución ──────
def mostrar_balance(y, nombre, encoder):
    distrib = pd.Series(y).value_counts().sort_index()
    etiquetas = encoder.inverse_transform(distrib.index)
    df_balance = pd.DataFrame({"Etiqueta": etiquetas, "Cantidad": distrib.values})
    print(f"\nDistribución en {nombre}:")
    display(df_balance)

# Verificación de balance por set
mostrar_balance(y_tfidf_train, "TF-IDF (Train)", le_tfidf)
mostrar_balance(y_tfidf_test, "TF-IDF (Test)", le_tfidf)
mostrar_balance(y_use_train, "MiniLM (Train)", le_use)
mostrar_balance(y_use_test, "MiniLM (Test)", le_use)

"""#5. Implementación de algoritmos de clasificación en Python

###5.1 KNN con TF-IDF
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_fscore_support, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

valores_k = [3, 5, 7, 9]
metricas = ['euclidean']
resultados_knn = []

# Binarizar etiquetas para ROC-AUC multiclase
y_test_bin = label_binarize(y_tfidf_test, classes=np.unique(y_tfidf_test))
n_classes = y_test_bin.shape[1]

for metric in metricas:
    for k in valores_k:
        knn = KNeighborsClassifier(n_neighbors=k, metric=metric)
        knn.fit(X_tfidf_train, y_tfidf_train)
        y_pred = knn.predict(X_tfidf_test)

        # Métricas globales
        acc = accuracy_score(y_tfidf_test, y_pred)
        prec = precision_score(y_tfidf_test, y_pred, average='weighted', zero_division=0)
        rec = recall_score(y_tfidf_test, y_pred, average='weighted')
        f1 = f1_score(y_tfidf_test, y_pred, average='weighted')

        # ROC-AUC
        try:
            y_prob = knn.predict_proba(X_tfidf_test)
            auc_score = roc_auc_score(y_test_bin, y_prob, average="macro", multi_class="ovr")
        except:
            auc_score = np.nan

        resultados_knn.append((k, metric, acc, prec, rec, f1, auc_score))

        print(f"\n--- K = {k} | Distancia = {metric} ---")
        print(f"Accuracy:  {acc:.4f}")
        print(f"Precision: {prec:.4f}")
        print(f"Recall:    {rec:.4f}")
        print(f"F1-score:  {f1:.4f}")
        print(f"ROC-AUC:   {auc_score:.4f}")
        print("\nReporte de clasificación:")
        print(classification_report(y_tfidf_test, y_pred, target_names=le_tfidf.classes_))

        # ───────────── Matriz de Confusión (Normalizada) ─────────────
        cm = confusion_matrix(y_tfidf_test, y_pred, normalize='true')
        plt.figure(figsize=(6, 5))
        sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues", xticklabels=le_tfidf.classes_, yticklabels=le_tfidf.classes_)
        plt.title(f"Matriz de Confusión Normalizada (K = {k})")
        plt.xlabel("Predicción")
        plt.ylabel("Real")
        plt.tight_layout()
        plt.show()
        print()

        # ───────────── Métricas por Clase (Precision, Recall, F1) ─────────────
        prec_c, rec_c, f1_c, _ = precision_recall_fscore_support(y_tfidf_test, y_pred, zero_division=0)
        df_clase = pd.DataFrame({
            "Emoción": le_tfidf.classes_,
            "Precisión": prec_c,
            "Exhaustividad": rec_c,
            "F1-score": f1_c
        }).set_index("Emoción")

        df_clase.plot(kind='bar', figsize=(10, 6), colormap="Set2")
        plt.title(f"Métricas por clase (K = {k})")
        plt.ylim(0, 1)
        plt.ylabel("Valor")
        plt.grid(True)
        plt.tight_layout()
        plt.show()
        print()

        # ───────────── Curvas ROC por clase ─────────────
        if 'y_prob' in locals():
            plt.figure(figsize=(10, 8))
            for i in range(n_classes):
                fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
                auc_i = auc(fpr, tpr)
                plt.plot(fpr, tpr, label=f"{le_tfidf.classes_[i]} (AUC = {auc_i:.2f})")

            plt.plot([0, 1], [0, 1], 'k--')
            plt.title(f"Curvas ROC por clase (K = {k})")
            plt.xlabel("FPR")
            plt.ylabel("TPR")
            plt.legend()
            plt.grid(True)
            plt.tight_layout()
            plt.show()
            print()

# ───────────── Tabla resumen global ─────────────
df_resultados_knn = pd.DataFrame(resultados_knn, columns=["K", "Métrica", "Accuracy", "Precision", "Recall", "F1-score", "ROC-AUC"])
print("\nResumen comparativo de KNN:")
display(df_resultados_knn.sort_values(by="F1-score", ascending=False))
print()

# ───────────── Gráfico resumen final ─────────────
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_resultados_knn, x="K", y="F1-score", marker="o", label="F1-score")
sns.lineplot(data=df_resultados_knn, x="K", y="ROC-AUC", marker="o", label="ROC-AUC")
plt.title("Comparación de F1-score y ROC-AUC por valor de K (KNN)")
plt.ylabel("Puntaje")
plt.grid(True)
plt.tight_layout()
plt.show()

"""###5.2 Naive Bayes con TF-IDF"""

from sklearn.naive_bayes import MultinomialNB, ComplementNB
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_fscore_support, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Etiquetas binarizadas para ROC-AUC multiclase
y_test_bin = label_binarize(y_tfidf_test, classes=np.unique(y_tfidf_test))
n_classes = y_test_bin.shape[1]

# Modelos a probar
modelos_nb = [
    ("MultinomialNB α=1.0", MultinomialNB(alpha=1.0)),
    ("MultinomialNB α=0.5", MultinomialNB(alpha=0.5)),
]

resultados_nb = []
modelos_entrenados = []

for nombre, modelo in modelos_nb:
    modelo.fit(X_tfidf_train, y_tfidf_train)
    modelos_entrenados.append((nombre, modelo))  # Guardar el modelo entrenado
    y_pred = modelo.predict(X_tfidf_test)

    acc = accuracy_score(y_tfidf_test, y_pred)
    prec = precision_score(y_tfidf_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_tfidf_test, y_pred, average='weighted')
    f1 = f1_score(y_tfidf_test, y_pred, average='weighted')

    try:
        y_prob = modelo.predict_proba(X_tfidf_test)
        auc_score = roc_auc_score(y_test_bin, y_prob, average="macro", multi_class="ovr")
    except:
        auc_score = np.nan

    resultados_nb.append((nombre, acc, prec, rec, f1, auc_score))

    print(f"\n--- {nombre} ---")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-score:  {f1:.4f}")
    print(f"ROC-AUC:   {auc_score:.4f}")
    print("\nReporte de clasificación:")
    print(classification_report(y_tfidf_test, y_pred, target_names=le_tfidf.classes_))

    # Matriz de confusión normalizada
    cm = confusion_matrix(y_tfidf_test, y_pred, normalize='true')
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues", xticklabels=le_tfidf.classes_, yticklabels=le_tfidf.classes_)
    plt.title(f"Matriz de Confusión Normalizada – {nombre}")
    plt.xlabel("Predicción")
    plt.ylabel("Real")
    plt.tight_layout()
    plt.show()
    print()

    # Métricas por clase
    prec_c, rec_c, f1_c, _ = precision_recall_fscore_support(y_tfidf_test, y_pred, zero_division=0)
    df_clase = pd.DataFrame({
        "Emoción": le_tfidf.classes_,
        "Precisión": prec_c,
        "Exhaustividad": rec_c,
        "F1-score": f1_c
    }).set_index("Emoción")

    df_clase.plot(kind='bar', figsize=(10, 6), colormap="Set2")
    plt.title(f"Métricas por clase – {nombre}")
    plt.ylim(0, 1)
    plt.ylabel("Valor")
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

    # Curvas ROC por clase
    if 'y_prob' in locals():
        plt.figure(figsize=(10, 8))
        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
            auc_i = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f"{le_tfidf.classes_[i]} (AUC = {auc_i:.2f})")

        plt.plot([0, 1], [0, 1], 'k--')
        plt.title(f"Curvas ROC por clase – {nombre}")
        plt.xlabel("FPR")
        plt.ylabel("TPR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()
        print()

# Tabla resumen final
df_resultados_nb = pd.DataFrame(resultados_nb, columns=["Modelo", "Accuracy", "Precision", "Recall", "F1-score", "ROC-AUC"])
df_resultados_nb.sort_values(by="F1-score", ascending=False, inplace=True)
display(df_resultados_nb)
print()

# Comparación visual
plt.figure(figsize=(10, 6))
sns.barplot(data=df_resultados_nb, x="Modelo", y="F1-score", hue="Modelo", palette="viridis", legend=False)
plt.title("Comparación de F1-score – Naive Bayes (TF-IDF)")
plt.ylabel("F1-score")
plt.xticks(rotation=45)
plt.ylim(0, 1)
plt.tight_layout()
plt.grid(True)
plt.show()

# ──────────────────────────────────────────────────────
# GUARDAR EL MEJOR MODELO ENTRENADO SEGÚN F1-SCORE
# ──────────────────────────────────────────────────────
import joblib

# Obtener nombre del mejor modelo
mejor_nombre = df_resultados_nb.iloc[0]["Modelo"]
# Obtener el objeto modelo desde modelos_entrenados
mejor_modelo = dict(modelos_entrenados)[mejor_nombre]

# Guardar modelo, vectorizador y codificador
joblib.dump(mejor_modelo, "modelo_nb_tfidf.pkl")
joblib.dump(vectorizador, "vectorizador_tfidf.pkl")
joblib.dump(le_tfidf, "label_encoder_tfidf.pkl")

print(f"\nModelo guardado: {mejor_nombre} como 'modelo_nb_tfidf.pkl'")

"""###5.3 SVM con TF-IDF"""

from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_fscore_support, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Binarizar etiquetas para ROC-AUC
y_test_bin = label_binarize(y_tfidf_test, classes=np.unique(y_tfidf_test))
n_classes = y_test_bin.shape[1]

# Configuraciones a evaluar
parametros_svm = [
    ("C=0.1", SVC(kernel='linear', C=0.1, probability=True, random_state=42)),
    ("C=1.0 (balanced)", SVC(kernel='linear', C=1.0, class_weight='balanced', probability=True, random_state=42)),
]

resultados_svm = []

for nombre, modelo in parametros_svm:
    pipeline = make_pipeline(
        StandardScaler(with_mean=False),  # porque TF-IDF es sparse
        modelo
    )
    pipeline.fit(X_tfidf_train, y_tfidf_train)
    y_pred = pipeline.predict(X_tfidf_test)

    # Métricas globales
    acc = accuracy_score(y_tfidf_test, y_pred)
    prec = precision_score(y_tfidf_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_tfidf_test, y_pred, average='weighted')
    f1 = f1_score(y_tfidf_test, y_pred, average='weighted')

    try:
        y_prob = pipeline.predict_proba(X_tfidf_test)
        auc_score = roc_auc_score(y_test_bin, y_prob, average="macro", multi_class="ovr")
    except:
        auc_score = np.nan

    resultados_svm.append((nombre, acc, prec, rec, f1, auc_score))

    print(f"\n--- {nombre} ---")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-score:  {f1:.4f}")
    print(f"ROC-AUC:   {auc_score:.4f}")
    print("\nReporte de clasificación:")
    print(classification_report(y_tfidf_test, y_pred, target_names=le_tfidf.classes_))
    print()

    # Matriz de Confusión Normalizada
    cm = confusion_matrix(y_tfidf_test, y_pred, normalize='true')
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues", xticklabels=le_tfidf.classes_, yticklabels=le_tfidf.classes_)
    plt.title(f"Matriz de Confusión Normalizada – {nombre}")
    plt.xlabel("Predicción")
    plt.ylabel("Real")
    plt.tight_layout()
    plt.show()
    print()

    # Métricas por clase
    prec_c, rec_c, f1_c, _ = precision_recall_fscore_support(y_tfidf_test, y_pred, zero_division=0)
    df_clase = pd.DataFrame({
        "Emoción": le_tfidf.classes_,
        "Precisión": prec_c,
        "Exhaustividad": rec_c,
        "F1-score": f1_c
    }).set_index("Emoción")

    df_clase.plot(kind='bar', figsize=(10, 6), colormap="Set2")
    plt.title(f"Métricas por clase – {nombre}")
    plt.ylim(0, 1)
    plt.ylabel("Valor")
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

    # Curvas ROC por clase
    if 'y_prob' in locals():
        plt.figure(figsize=(10, 8))
        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
            auc_i = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f"{le_tfidf.classes_[i]} (AUC = {auc_i:.2f})")

        plt.plot([0, 1], [0, 1], 'k--')
        plt.title(f"Curvas ROC por clase – {nombre}")
        plt.xlabel("FPR")
        plt.ylabel("TPR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()
        print()

# ─── Comparación final ───
df_svm = pd.DataFrame(resultados_svm, columns=["Configuración", "Accuracy", "Precision", "Recall", "F1-score", "ROC-AUC"])
df_svm.sort_values(by="F1-score", ascending=False, inplace=True)

# Mostrar tabla comparativa
display(df_svm)

# Gráfico combinado F1-score y ROC-AUC (estilo KNN)
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_svm, x="Configuración", y="F1-score", label="F1-score", marker="o", linewidth=2)
sns.lineplot(data=df_svm, x="Configuración", y="ROC-AUC", label="ROC-AUC", marker="o", linewidth=2)
plt.title("Comparación de F1-score y ROC-AUC – SVM (TF-IDF)")
plt.ylabel("Puntaje")
plt.ylim(0.5, 1)
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.legend()
plt.show()

"""###5.4. Random Forest con TF-IDF"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_fscore_support, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Binarizar etiquetas para ROC-AUC
y_test_bin = label_binarize(y_tfidf_test, classes=np.unique(y_tfidf_test))
n_classes = y_test_bin.shape[1]

# ───── Configuraciones de Random Forest ─────
modelos_rf_tfidf = [
    ("RF base (100 árboles)", RandomForestClassifier(
        n_estimators=100, max_depth=None, random_state=42, n_jobs=-1)),

    ("RF poda leve", RandomForestClassifier(
        n_estimators=100, max_depth=10, min_samples_split=5, random_state=42, n_jobs=-1)),

    ("RF profundo (200 árboles)", RandomForestClassifier(
        n_estimators=200, max_depth=None, random_state=42, n_jobs=-1))
]

resultados_rf_tfidf = []

for nombre, modelo in modelos_rf_tfidf:
    modelo.fit(X_tfidf_train, y_tfidf_train)
    y_pred = modelo.predict(X_tfidf_test)
    y_prob = modelo.predict_proba(X_tfidf_test)

    acc = accuracy_score(y_tfidf_test, y_pred)
    prec = precision_score(y_tfidf_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_tfidf_test, y_pred, average='weighted')
    f1 = f1_score(y_tfidf_test, y_pred, average='weighted')
    auc_score = roc_auc_score(y_test_bin, y_prob, average="macro", multi_class="ovr")

    resultados_rf_tfidf.append((nombre, acc, prec, rec, f1, auc_score))

    print(f"\n--- {nombre} ---")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-score:  {f1:.4f}")
    print(f"ROC-AUC:   {auc_score:.4f}")
    print("\nReporte de clasificación:")
    print(classification_report(y_tfidf_test, y_pred, target_names=le_tfidf.classes_))

    # Matriz de Confusión Normalizada
    cm = confusion_matrix(y_tfidf_test, y_pred, normalize='true')
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="YlGnBu", xticklabels=le_tfidf.classes_, yticklabels=le_tfidf.classes_)
    plt.title(f"Matriz de Confusión Normalizada – {nombre}")
    plt.xlabel("Predicción")
    plt.ylabel("Real")
    plt.tight_layout()
    plt.show()
    print()

    # Métricas por clase
    prec_c, rec_c, f1_c, _ = precision_recall_fscore_support(y_tfidf_test, y_pred, zero_division=0)
    df_clase = pd.DataFrame({
        "Emoción": le_tfidf.classes_,
        "Precisión": prec_c,
        "Exhaustividad": rec_c,
        "F1-score": f1_c
    }).set_index("Emoción")

    df_clase.plot(kind='bar', figsize=(10, 6), colormap="Set2")
    plt.title(f"Métricas por clase – {nombre}")
    plt.ylim(0, 1)
    plt.ylabel("Valor")
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

    # Curvas ROC por clase
    plt.figure(figsize=(10, 8))
    for i in range(n_classes):
        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
        auc_i = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{le_tfidf.classes_[i]} (AUC = {auc_i:.2f})")

    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f"Curvas ROC por clase – {nombre}")
    plt.xlabel("FPR")
    plt.ylabel("TPR")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

# ───── Tabla resumen ─────
df_rf_tfidf = pd.DataFrame(resultados_rf_tfidf, columns=["Modelo", "Accuracy", "Precision", "Recall", "F1-score", "ROC-AUC"])
df_rf_tfidf.sort_values(by="F1-score", ascending=False, inplace=True)
display(df_rf_tfidf)
print()

# ───── Gráfico comparativo final ─────
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_rf_tfidf, x="Modelo", y="F1-score", label="F1-score", marker="o")
sns.lineplot(data=df_rf_tfidf, x="Modelo", y="ROC-AUC", label="ROC-AUC", marker="o")
plt.title("Random Forest con TF-IDF: F1-score vs ROC-AUC")
plt.ylabel("Puntaje")
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.legend()
plt.show()

"""###5.5. KNN usando Embeddings"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_fscore_support, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Etiquetas binarizadas para ROC-AUC multiclase
y_test_bin = label_binarize(y_use_test, classes=np.unique(y_use_test))
n_classes = y_test_bin.shape[1]

valores_k = [3, 5, 7, 9]
resultados_knn_use = []

for k in valores_k:
    knn = KNeighborsClassifier(n_neighbors=k, metric='cosine', weights='distance')
    knn.fit(X_use_train, y_use_train)
    y_pred = knn.predict(X_use_test)

    acc = accuracy_score(y_use_test, y_pred)
    prec = precision_score(y_use_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_use_test, y_pred, average='weighted')
    f1 = f1_score(y_use_test, y_pred, average='weighted')

    try:
        y_prob = knn.predict_proba(X_use_test)
        auc_score = roc_auc_score(y_test_bin, y_prob, average="macro", multi_class="ovr")
    except:
        auc_score = np.nan

    resultados_knn_use.append((k, acc, prec, rec, f1, auc_score))

    print(f"\n--- K = {k} ---")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-score:  {f1:.4f}")
    print(f"ROC-AUC:   {auc_score:.4f}")
    print("\nReporte de clasificación:")
    print(classification_report(y_use_test, y_pred, target_names=le_use.classes_))

    # Matriz de Confusión
    cm = confusion_matrix(y_use_test, y_pred, normalize='true')
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="Blues", xticklabels=le_use.classes_, yticklabels=le_use.classes_)
    plt.title(f"Matriz de Confusión Normalizada – K = {k}")
    plt.xlabel("Predicción")
    plt.ylabel("Real")
    plt.tight_layout()
    plt.show()
    print()

    # Métricas por clase
    prec_c, rec_c, f1_c, _ = precision_recall_fscore_support(y_use_test, y_pred, zero_division=0)
    df_clase = pd.DataFrame({
        "Emoción": le_use.classes_,
        "Precisión": prec_c,
        "Exhaustividad": rec_c,
        "F1-score": f1_c
    }).set_index("Emoción")

    df_clase.plot(kind='bar', figsize=(10, 6), colormap="Set2")
    plt.title(f"Métricas por clase – K = {k}")
    plt.ylim(0, 1)
    plt.ylabel("Valor")
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

    # Curvas ROC por clase
    if 'y_prob' in locals():
        plt.figure(figsize=(10, 8))
        for i in range(n_classes):
            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
            auc_i = auc(fpr, tpr)
            plt.plot(fpr, tpr, label=f"{le_use.classes_[i]} (AUC = {auc_i:.2f})")

        plt.plot([0, 1], [0, 1], 'k--')
        plt.title(f"Curvas ROC por clase – K = {k}")
        plt.xlabel("FPR")
        plt.ylabel("TPR")
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()
        print()

# Crear DataFrame resumen
df_knn_use = pd.DataFrame(resultados_knn_use, columns=["K", "Accuracy", "Precision", "Recall", "F1-score", "ROC-AUC"])
df_knn_use.sort_values(by="F1-score", ascending=False, inplace=True)
print()

# Tabla resumen
print("\nComparación de KNN con MiniLM:")
display(df_knn_use)
print()

# Gráfico final estilo KNN/SVM
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_knn_use, x="K", y="F1-score", label="F1-score", marker="o", linewidth=2)
sns.lineplot(data=df_knn_use, x="K", y="ROC-AUC", label="ROC-AUC", marker="o", linewidth=2)
plt.title("KNN con Embeddings MiniLM: Comparación de F1-score y ROC-AUC")
plt.ylabel("Puntaje")
plt.ylim(0.5, 1)
plt.grid(True)
plt.tight_layout()
plt.legend()
plt.show()

"""###5.6. Naive Bayes (Gaussian) con embeddings"""

from sklearn.naive_bayes import GaussianNB
from sklearn.decomposition import PCA
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_fscore_support, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Binarizar etiquetas para ROC-AUC
y_test_bin = label_binarize(y_use_test, classes=np.unique(y_use_test))
n_classes = y_test_bin.shape[1]

# ───── Configuraciones a evaluar ─────
modelos_nb_emb = [
    ("GaussianNB base", GaussianNB(), X_use_train, X_use_test),

    ("GaussianNB smoothing=1e-6", GaussianNB(var_smoothing=1e-6), X_use_train, X_use_test)
]

resultados_nb_emb = []

for nombre, modelo, X_train_, X_test_ in modelos_nb_emb:
    modelo.fit(X_train_, y_use_train)
    y_pred = modelo.predict(X_test_)
    y_prob = modelo.predict_proba(X_test_)

    acc = accuracy_score(y_use_test, y_pred)
    prec = precision_score(y_use_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_use_test, y_pred, average='weighted')
    f1 = f1_score(y_use_test, y_pred, average='weighted')
    auc_score = roc_auc_score(y_test_bin, y_prob, average="macro", multi_class="ovr")

    resultados_nb_emb.append((nombre, acc, prec, rec, f1, auc_score))

    print(f"\n--- {nombre} ---")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-score:  {f1:.4f}")
    print(f"ROC-AUC:   {auc_score:.4f}")
    print("\nReporte de clasificación:")
    print(classification_report(y_use_test, y_pred, target_names=le_use.classes_))

    # Matriz de Confusión
    cm = confusion_matrix(y_use_test, y_pred, normalize='true')
    plt.figure(figsize=(6,5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="Oranges", xticklabels=le_use.classes_, yticklabels=le_use.classes_)
    plt.title(f"Matriz de Confusión Normalizada – {nombre}")
    plt.xlabel("Predicción")
    plt.ylabel("Real")
    plt.tight_layout()
    plt.show()
    print()

    # Métricas por clase
    prec_c, rec_c, f1_c, _ = precision_recall_fscore_support(y_use_test, y_pred, zero_division=0)
    df_clase = pd.DataFrame({
        "Emoción": le_use.classes_,
        "Precisión": prec_c,
        "Exhaustividad": rec_c,
        "F1-score": f1_c
    }).set_index("Emoción")

    df_clase.plot(kind='bar', figsize=(10, 6), colormap="Set2")
    plt.title(f"Métricas por clase – {nombre}")
    plt.ylim(0, 1)
    plt.ylabel("Valor")
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

    # Curvas ROC
    plt.figure(figsize=(10, 8))
    for i in range(n_classes):
        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
        auc_i = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{le_use.classes_[i]} (AUC = {auc_i:.2f})")

    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f"Curvas ROC por clase – {nombre}")
    plt.xlabel("FPR")
    plt.ylabel("TPR")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

# ───── Tabla resumen ─────
df_nb_emb = pd.DataFrame(resultados_nb_emb, columns=["Modelo", "Accuracy", "Precision", "Recall", "F1-score", "ROC-AUC"])
df_nb_emb.sort_values(by="F1-score", ascending=False, inplace=True)
display(df_nb_emb)
print()

# ───── Gráfico comparativo F1 vs ROC-AUC ─────
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_nb_emb, x="Modelo", y="F1-score", label="F1-score", marker="o", linewidth=2)
sns.lineplot(data=df_nb_emb, x="Modelo", y="ROC-AUC", label="ROC-AUC", marker="o", linewidth=2)
plt.title("Naive Bayes con MiniLM: F1-score vs ROC-AUC")
plt.ylabel("Puntaje")
plt.xticks(rotation=45)
plt.ylim(0.5, 1)
plt.grid(True)
plt.tight_layout()
plt.legend()
plt.show()

"""###5.7. Clasificación con SVM usando Embeddings"""

from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_fscore_support, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Binarizar etiquetas para ROC-AUC multiclase
y_test_bin = label_binarize(y_use_test, classes=np.unique(y_use_test))
n_classes = y_test_bin.shape[1]

# ─── Variantes a evaluar ───
modelos_svm = [
    ("SVM C=0.1", SVC(kernel='linear', C=0.1, probability=True, random_state=42)),
    ("SVM C=1.0 (balanced)", SVC(kernel='linear', C=1.0, class_weight='balanced', probability=True, random_state=42)),
]

resultados_svm_emb = []
pipelines_entrenados = []

for nombre, modelo in modelos_svm:
    pipeline = make_pipeline(
        StandardScaler(),
        modelo
    )
    pipeline.fit(X_use_train, y_use_train)
    pipelines_entrenados.append((nombre, pipeline))  # Guardar el pipeline con nombre

    y_pred = pipeline.predict(X_use_test)
    y_prob = pipeline.predict_proba(X_use_test)

    acc = accuracy_score(y_use_test, y_pred)
    prec = precision_score(y_use_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_use_test, y_pred, average='weighted')
    f1 = f1_score(y_use_test, y_pred, average='weighted')
    auc_score = roc_auc_score(y_test_bin, y_prob, average="macro", multi_class="ovr")

    resultados_svm_emb.append((nombre, acc, prec, rec, f1, auc_score))

    print(f"\n--- {nombre} ---")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-score:  {f1:.4f}")
    print(f"ROC-AUC:   {auc_score:.4f}")
    print("\nReporte de clasificación:")
    print(classification_report(y_use_test, y_pred, target_names=le_use.classes_))

    cm = confusion_matrix(y_use_test, y_pred, normalize='true')
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="PuBuGn", xticklabels=le_use.classes_, yticklabels=le_use.classes_)
    plt.title(f"Matriz de Confusión Normalizada – {nombre}")
    plt.xlabel("Predicción")
    plt.ylabel("Real")
    plt.tight_layout()
    plt.show()
    print()

    prec_c, rec_c, f1_c, _ = precision_recall_fscore_support(y_use_test, y_pred, zero_division=0)
    df_clase = pd.DataFrame({
        "Emoción": le_use.classes_,
        "Precisión": prec_c,
        "Exhaustividad": rec_c,
        "F1-score": f1_c
    }).set_index("Emoción")

    df_clase.plot(kind='bar', figsize=(10, 6), colormap="Set2")
    plt.title(f"Métricas por clase – {nombre}")
    plt.ylim(0, 1)
    plt.ylabel("Valor")
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

    plt.figure(figsize=(10, 8))
    for i in range(n_classes):
        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
        auc_i = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{le_use.classes_[i]} (AUC = {auc_i:.2f})")

    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f"Curvas ROC por clase – {nombre}")
    plt.xlabel("FPR")
    plt.ylabel("TPR")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

# ─── Comparación final ───
df_svm_emb = pd.DataFrame(resultados_svm_emb, columns=["Modelo", "Accuracy", "Precision", "Recall", "F1-score", "ROC-AUC"])
df_svm_emb.sort_values(by="F1-score", ascending=False, inplace=True)
print()

# Tabla
display(df_svm_emb)
print()

# Gráfico comparativo
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_svm_emb, x="Modelo", y="F1-score", label="F1-score", marker="o", linewidth=2)
sns.lineplot(data=df_svm_emb, x="Modelo", y="ROC-AUC", label="ROC-AUC", marker="o", linewidth=2)
plt.title("SVM con Embeddings (MiniLM): F1-score vs ROC-AUC")
plt.ylabel("Puntaje")
plt.ylim(0.5, 1)
plt.xticks(rotation=45)
plt.grid(True)
plt.tight_layout()
plt.legend()
plt.show()

# ──────────────────────────────────────────────────────
# GUARDAR EL MEJOR MODELO ENTRENADO SEGÚN F1-SCORE
# ──────────────────────────────────────────────────────
import joblib

mejor_nombre = df_svm_emb.iloc[0]["Modelo"]
mejor_pipeline = dict(pipelines_entrenados)[mejor_nombre]

joblib.dump(mejor_pipeline, "modelo_svm_embeddings.pkl")
joblib.dump(le_use, "label_encoder_embeddings.pkl")

print(f"\nModelo guardado: {mejor_nombre} como 'modelo_svm_embeddings.pkl'")

"""###5.8. Random Forest con Embeddings"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_fscore_support, roc_curve, auc
)
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np

# Binarizar etiquetas para ROC-AUC
y_test_bin = label_binarize(y_use_test, classes=np.unique(y_use_test))
n_classes = y_test_bin.shape[1]

# ───── Configuraciones de Random Forest ─────
modelos_rf = [
    ("RF base (100 árboles)", RandomForestClassifier(
        n_estimators=100, max_depth=None, random_state=42, n_jobs=-1)),

    ("RF poda leve", RandomForestClassifier(
        n_estimators=100, max_depth=10, min_samples_split=5, random_state=42, n_jobs=-1)),

    ("RF profundo (200 árboles)", RandomForestClassifier(
        n_estimators=200, max_depth=None, random_state=42, n_jobs=-1))
]

resultados_rf_emb = []

for nombre, modelo in modelos_rf:
    modelo.fit(X_use_train, y_use_train)
    y_pred = modelo.predict(X_use_test)
    y_prob = modelo.predict_proba(X_use_test)

    acc = accuracy_score(y_use_test, y_pred)
    prec = precision_score(y_use_test, y_pred, average='weighted', zero_division=0)
    rec = recall_score(y_use_test, y_pred, average='weighted')
    f1 = f1_score(y_use_test, y_pred, average='weighted')
    auc_score = roc_auc_score(y_test_bin, y_prob, average="macro", multi_class="ovr")

    resultados_rf_emb.append((nombre, acc, prec, rec, f1, auc_score))

    print(f"\n--- {nombre} ---")
    print(f"Accuracy:  {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall:    {rec:.4f}")
    print(f"F1-score:  {f1:.4f}")
    print(f"ROC-AUC:   {auc_score:.4f}")
    print("\nReporte de clasificación:")
    print(classification_report(y_use_test, y_pred, target_names=le_use.classes_))

    # ───── Matriz de Confusión ─────
    cm = confusion_matrix(y_use_test, y_pred, normalize='true')
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt=".2f", cmap="YlOrBr", xticklabels=le_use.classes_, yticklabels=le_use.classes_)
    plt.title(f"Matriz de Confusión Normalizada – {nombre}")
    plt.xlabel("Predicción")
    plt.ylabel("Real")
    plt.tight_layout()
    plt.show()
    print()

    # ───── Métricas por clase ─────
    prec_c, rec_c, f1_c, _ = precision_recall_fscore_support(y_use_test, y_pred, zero_division=0)
    df_clase = pd.DataFrame({
        "Emoción": le_use.classes_,
        "Precisión": prec_c,
        "Exhaustividad": rec_c,
        "F1-score": f1_c
    }).set_index("Emoción")

    df_clase.plot(kind='bar', figsize=(10, 6), colormap="Set2")
    plt.title(f"Métricas por clase – {nombre}")
    plt.ylim(0, 1)
    plt.ylabel("Valor")
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

    # ───── Curvas ROC por clase ─────
    plt.figure(figsize=(10, 8))
    for i in range(n_classes):
        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_prob[:, i])
        auc_i = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{le_use.classes_[i]} (AUC = {auc_i:.2f})")

    plt.plot([0, 1], [0, 1], 'k--')
    plt.title(f"Curvas ROC por clase – {nombre}")
    plt.xlabel("FPR")
    plt.ylabel("TPR")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()
    print()

# ───── Tabla resumen ─────
df_rf_emb = pd.DataFrame(resultados_rf_emb, columns=["Modelo", "Accuracy", "Precision", "Recall", "F1-score", "ROC-AUC"])
df_rf_emb.sort_values(by="F1-score", ascending=False, inplace=True)
display(df_rf_emb)
print()

# ───── Gráfico final: F1 vs ROC-AUC ─────
plt.figure(figsize=(10, 6))
sns.lineplot(data=df_rf_emb, x="Modelo", y="F1-score", label="F1-score", marker="o", linewidth=2)
sns.lineplot(data=df_rf_emb, x="Modelo", y="ROC-AUC", label="ROC-AUC", marker="o", linewidth=2)
plt.title("Random Forest con Embeddings (MiniLM): F1-score vs ROC-AUC")
plt.ylabel("Puntaje")
plt.xticks(rotation=45)
plt.ylim(0.5, 1)
plt.grid(True)
plt.tight_layout()
plt.legend()
plt.show()

"""#6. Validación de Modelos

##6.1 Aplicación de Validación Cruzada
"""

from sklearn.model_selection import KFold, cross_val_score
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer, f1_score, accuracy_score, precision_score, recall_score
import numpy as np
import pandas as pd

# Modelos a evaluar
model_nb_tfidf = MultinomialNB(alpha=0.5)
model_svm_embeddings = SVC(kernel='linear', C=0.1, probability=True, random_state=42)
model_rf_embeddings = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, n_jobs=-1)

# Scorer para F1 ponderado
f1_scorer = make_scorer(f1_score, average='weighted', zero_division=0)

# Configuración de validación cruzada
k_values = [3, 5, 10]

# ────────────────────────────────
# Validación cruzada NB (TF-IDF)
# ────────────────────────────────
print("\n• Validación cruzada para Naive Bayes (TF-IDF)...")
cv_results_nb = {}

for k in k_values:
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    f1_scores = cross_val_score(model_nb_tfidf, X_tfidf, y_tfidf, cv=kf, scoring=f1_scorer)
    acc_scores = cross_val_score(model_nb_tfidf, X_tfidf, y_tfidf, cv=kf, scoring='accuracy')
    cv_results_nb[k] = {
        'mean_f1': np.mean(f1_scores),
        'std_f1': np.std(f1_scores),
        'mean_accuracy': np.mean(acc_scores),
        'std_accuracy': np.std(acc_scores)
    }
    print(f"  - NB (TF-IDF) con k={k}: F1 = {cv_results_nb[k]['mean_f1']:.4f} (±{cv_results_nb[k]['std_f1']:.4f}), "
          f"Acc = {cv_results_nb[k]['mean_accuracy']:.4f} (±{cv_results_nb[k]['std_accuracy']:.4f})")

# ────────────────────────────────
# Validación cruzada SVM (Embeddings)
# ────────────────────────────────
print("\n• Validación cruzada para SVM (Embeddings)...")
cv_results_svm = {}

for k in k_values:
    kf = KFold(n_splits=k, shuffle=True, random_state=42)
    f1_scores = cross_val_score(model_svm_embeddings, X_use, y_use, cv=kf, scoring=f1_scorer)
    acc_scores = cross_val_score(model_svm_embeddings, X_use, y_use, cv=kf, scoring='accuracy')
    cv_results_svm[k] = {
        'mean_f1': np.mean(f1_scores),
        'std_f1': np.std(f1_scores),
        'mean_accuracy': np.mean(acc_scores),
        'std_accuracy': np.std(acc_scores)
    }
    print(f"  - SVM (Embeddings) con k={k}: F1 = {cv_results_svm[k]['mean_f1']:.4f} (±{cv_results_svm[k]['std_f1']:.4f}), "
          f"Acc = {cv_results_svm[k]['mean_accuracy']:.4f} (±{cv_results_svm[k]['std_accuracy']:.4f})")

"""##6.2 Comparativa de Resultados"""

# --------------------------------------------------------------------------------------------------
# Acciones: Comparar los resultados de la validación cruzada con el conjunto de prueba
# --------------------------------------------------------------------------------------------------
print("\n" + "=" * 80)
print(" COMPARACIÓN ENTRE VALIDACIÓN CRUZADA Y CONJUNTO DE PRUEBA".center(80))
print("=" * 80)

# Resultados de prueba
test_f1_nb_tfidf = 0.7696
test_roc_auc_nb_tfidf = 0.9514

test_f1_svm_embeddings = 0.7558
test_roc_auc_svm_embeddings = 0.9503

# ─────────────────────────────────────────────
# Naive Bayes con TF-IDF
# ─────────────────────────────────────────────
print("\n" + "-" * 80)
print("MODELO: NAIVE BAYES (TF-IDF)".center(80))
print("-" * 80)

for k in k_values:
    cv_f1 = cv_results_nb[k]['mean_f1']
    delta = cv_f1 - test_f1_nb_tfidf

    print(f"k = {k}")
    print(f"  CV F1-score promedio : {cv_f1:.4f}")
    print(f"  Test F1-score        : {test_f1_nb_tfidf:.4f}")
    print(f"  Diferencia F1        : {delta:+.4f}")
    print(f"  Test ROC-AUC         : {test_roc_auc_nb_tfidf:.4f}")

    if abs(delta) > 0.05:
        print("  ↪ Advertencia: diferencia significativa entre CV y Test F1-score")
    else:
        print("  ↪ F1-score CV y Test son consistentes.")

    print("-" * 80)

# ─────────────────────────────────────────────
# SVM con Embeddings
# ─────────────────────────────────────────────
print("\n" + "-" * 80)
print("MODELO: SVM (EMBEDDINGS)".center(80))
print("-" * 80)

for k in k_values:
    cv_f1 = cv_results_svm[k]['mean_f1']
    delta = cv_f1 - test_f1_svm_embeddings

    print(f"k = {k}")
    print(f"  CV F1-score promedio : {cv_f1:.4f}")
    print(f"  Test F1-score        : {test_f1_svm_embeddings:.4f}")
    print(f"  Diferencia F1        : {delta:+.4f}")
    print(f"  Test ROC-AUC         : {test_roc_auc_svm_embeddings:.4f}")

    if abs(delta) > 0.05:
        print("  ↪ Advertencia: diferencia significativa entre CV y Test F1-score")
    else:
        print("  ↪ F1-score CV y Test son consistentes.")

    print("-" * 80)

# ─────────────────────────────────────────────
# Resumen final
# ─────────────────────────────────────────────
print("\n" + "=" * 80)
print(" ANÁLISIS DE GENERALIZACIÓN FINAL".center(80))
print("=" * 80)

# Naive Bayes
best_k_nb = k_values[np.argmax([cv_results_nb[k]['mean_f1'] for k in k_values])]
print("\nNaive Bayes (TF-IDF):")
print(f"  Mejor CV F1-score (k = {best_k_nb}) : {cv_results_nb[best_k_nb]['mean_f1']:.4f}")
print(f"  Test F1-score: {test_f1_nb_tfidf:.4f}")

# SVM
best_k_svm = k_values[np.argmax([cv_results_svm[k]['mean_f1'] for k in k_values])]
print("\nSVM (Embeddings):")
print(f"  Mejor CV F1-score (k = {best_k_svm}) : {cv_results_svm[best_k_svm]['mean_f1']:.4f}")
print(f"  Test F1-score: {test_f1_svm_embeddings:.4f}")

print("\n" + "=" * 80)

"""###6.3. Gráficas de Resultados"""

# --------------------------------------------------------------------------------------------------
# GRÁFICAS PARA ANÁLISIS DE RESULTADOS DE VALIDACIÓN CRUZADA
# --------------------------------------------------------------------------------------------------

print("\nGenerando gráficas para el análisis de los resultados de validación cruzada...\n")

# Preparar datos para las gráficas
data_nb = {
    'k': k_values,
    'F1-score (CV)': [cv_results_nb[k]['mean_f1'] for k in k_values],
    'Accuracy (CV)': [cv_results_nb[k]['mean_accuracy'] for k in k_values],
    'Modelo': 'Naive Bayes (TF-IDF)'
}
df_nb = pd.DataFrame(data_nb)

data_svm = {
    'k': k_values,
    'F1-score (CV)': [cv_results_svm[k]['mean_f1'] for k in k_values],
    'Accuracy (CV)': [cv_results_svm[k]['mean_accuracy'] for k in k_values],
    'Modelo': 'SVM (Embeddings)'
}
df_svm = pd.DataFrame(data_svm)

df_combined_cv = pd.concat([df_nb, df_svm])
print()

# 1. Gráfica de F1-score y Accuracy para Naive Bayes (TF-IDF)
plt.figure(figsize=(12, 6))
sns.lineplot(data=df_nb, x='k', y='F1-score (CV)', marker='o', label='F1-score CV', color='skyblue', linewidth=2)
sns.lineplot(data=df_nb, x='k', y='Accuracy (CV)', marker='s', label='Accuracy CV', color='lightcoral', linewidth=2)
plt.axhline(y=test_f1_nb_tfidf, color='blue', linestyle='--', label=f'F1-score Test ({test_f1_nb_tfidf:.4f})')
plt.title('Rendimiento de Naive Bayes (TF-IDF) en Validación Cruzada vs. Test', fontsize=16)
plt.xlabel('Número de Folds (k)', fontsize=12)
plt.ylabel('Métrica', fontsize=12)
plt.xticks(k_values)
plt.ylim(0.6, 1.0) # Ajustar límites para mejor visualización
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(title='Métrica', loc='lower right')
plt.tight_layout()
plt.show()
print()

# 2. Gráfica de F1-score y Accuracy para SVM (Embeddings)
plt.figure(figsize=(12, 6))
sns.lineplot(data=df_svm, x='k', y='F1-score (CV)', marker='o', label='F1-score CV', color='mediumseagreen', linewidth=2)
sns.lineplot(data=df_svm, x='k', y='Accuracy (CV)', marker='s', label='Accuracy CV', color='orange', linewidth=2)
plt.axhline(y=test_f1_svm_embeddings, color='darkgreen', linestyle='--', label=f'F1-score Test ({test_f1_svm_embeddings:.4f})')
plt.title('Rendimiento de SVM (Embeddings) en Validación Cruzada vs. Test', fontsize=16)
plt.xlabel('Número de Folds (k)', fontsize=12)
plt.ylabel('Métrica', fontsize=12)
plt.xticks(k_values)
plt.ylim(0.6, 1.0)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(title='Métrica', loc='lower right')
plt.tight_layout()
plt.show()
print()

# 3. Gráfica Comparativa de F1-score (CV promedio) entre ambos modelos y con F1-score de Test
df_plot_f1 = pd.DataFrame({
    'Modelo': ['Naive Bayes (TF-IDF)'] * len(k_values) + ['SVM (Embeddings)'] * len(k_values),
    'k': k_values * 2,
    'F1-score CV Promedio': [cv_results_nb[k]['mean_f1'] for k in k_values] + [cv_results_svm[k]['mean_f1'] for k in k_values]
})

plt.figure(figsize=(14, 7))
sns.barplot(data=df_plot_f1, x='k', y='F1-score CV Promedio', hue='Modelo', palette='viridis', alpha=0.8)

# Añadir líneas de F1-score del conjunto de prueba
plt.axhline(y=test_f1_nb_tfidf, color='darkblue', linestyle=':', label=f'NB F1-score Test ({test_f1_nb_tfidf:.4f})', linewidth=2)
plt.axhline(y=test_f1_svm_embeddings, color='purple', linestyle=':', label=f'SVM F1-score Test ({test_f1_svm_embeddings:.4f})', linewidth=2)

plt.title('Comparación de F1-score Promedio (CV) y F1-score (Test) por Modelo y K-fold', fontsize=16)
plt.xlabel('Número de Folds (k)', fontsize=12)
plt.ylabel('F1-score Promedio', fontsize=12)
plt.ylim(0.6, 1.0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.legend(title='Leyenda', loc='lower right')
plt.tight_layout()
plt.show()
print()

# 4. Gráfica de Desviación Estándar del F1-score (CV) para ver la consistencia
df_plot_std_f1 = pd.DataFrame({
    'Modelo': ['Naive Bayes (TF-IDF)'] * len(k_values) + ['SVM (Embeddings)'] * len(k_values),
    'k': k_values * 2,
    'Desviación Estándar F1 (CV)': [cv_results_nb[k]['std_f1'] for k in k_values] + [cv_results_svm[k]['std_f1'] for k in k_values]
})

plt.figure(figsize=(12, 6))
sns.lineplot(data=df_plot_std_f1, x='k', y='Desviación Estándar F1 (CV)', hue='Modelo', marker='o', palette='plasma', linewidth=2)
plt.title('Consistencia del F1-score en Validación Cruzada (Desviación Estándar)', fontsize=16)
plt.xlabel('Número de Folds (k)', fontsize=12)
plt.ylabel('Desviación Estándar del F1-score', fontsize=12)
plt.xticks(k_values)
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(title='Modelo', loc='upper left')
plt.tight_layout()
plt.show()

"""#7. Pruebas

###7.1. Pruebas con el modelo Naive Bayes (TF-IDF)
"""

import joblib

# Cargar modelo y herramientas
modelo = joblib.load("modelo_nb_tfidf.pkl")
vectorizador = joblib.load("vectorizador_tfidf.pkl")
le = joblib.load("label_encoder_tfidf.pkl")

# Textos optimizados por emoción (alineados al estilo de tu dataset)
textos = [
    "Cuando finalmente me abrazó ese día, sentí mucha alegría.",           # Felicidad
    "Lloré toda la noche por lo que pasó, me sentía tan cabizbajo.",       # Tristeza
    "Fue muy asqueroso lo que vi, no pude seguir comiendo.",               # Disgusto
    "Sentí mucha rabia dentro, me parecia injusto. ¡No lo merecía!",       # Ira
    "Estaba solo en casa y escuché pasos, me dieron escalofríos.",         # Miedo
    "Jamás imaginé eso, fue totalmente inesperado para mí."                # Sorpresa
]

# Predecir y mostrar
print("Predicciones ajustadas del modelo Naive Bayes (TF-IDF):\n")
for i, texto in enumerate(textos, start=1):
    texto_tfidf = vectorizador.transform([texto])
    pred = modelo.predict(texto_tfidf)
    emocion = le.inverse_transform(pred)[0]
    print(f"Texto {i}: {emocion}")

"""###7.2. Pruebas con el modelo SVM (Embeddings)"""

import joblib

# Cargar modelo y codificador
modelo = joblib.load("modelo_svm_embeddings.pkl")
le = joblib.load("label_encoder_embeddings.pkl")

# Textos ajustados por emoción esperada
textos = [
    "Ver a mi familia reunida me hizo sentir bien, fue un momento especial.",       # Felicidad
    "Lloré mucho ese día, me sentía totalmente solo y abandonado.",                 # Tristeza
    "Sentí un asco horrible al ver cómo se comportaban, fue repugnante.",           # Disgusto
    "Quiero hacer explotar todo, me da igual.",                                     # Ira
    "Escuché un ruido fuerte en la oscuridad y me congelé del susto.",              # Miedo
    "Nunca pensé que vendrían, es el regalo más grande que me han dado.",           # Sorpresa
]

# Predecir y mostrar
print("Predicciones del modelo SVM (Embeddings):\n")
for i, texto in enumerate(textos, start=1):
    emb = modelo_embeddings.encode([texto])
    pred = modelo.predict(emb)
    emocion = le.inverse_transform(pred)[0]
    print(f"Texto {i}: {emocion}")

"""#Extra - Guardado de Modelos"""

import shutil
import os

# Ruta de destino en tu Google Drive (ajusta si tu carpeta tiene otro nombre)
carpeta_drive = "/content/drive/My Drive/MineriaDatos/ProyectoFinal/ModelosGuardados"
os.makedirs(carpeta_drive, exist_ok=True)

# Archivos a mover al Drive
archivos_modelos = [
    "modelo_nb_tfidf.pkl",
    "vectorizador_tfidf.pkl",
    "label_encoder_tfidf.pkl",
    "modelo_svm_embeddings.pkl",
    "label_encoder_embeddings.pkl"
]

# Mover los archivos al Drive
for archivo in archivos_modelos:
    shutil.move(archivo, os.path.join(carpeta_drive, archivo))

print(f"Modelos guardados exitosamente en: {carpeta_drive}")